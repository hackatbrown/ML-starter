{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and Machine Learning Guide\n",
    "\n",
    "### Overview\n",
    "Welcome to the PyTorch Machine Learning Guide! In this notebook, we will dive into\n",
    "using PyTorch, a popular deep learning framework, to build and train machine learning\n",
    "models. Whether you are a beginner or looking to refresh your PyTorch knowledge,\n",
    "this guide will help you get started. \n",
    "\n",
    "Machine learning is a branch of artificial intelligence that empowers computers to learn from data and make predictions or decisions without explicit programming. It's like teaching a computer to recognize patterns, relationships, and trends within datasets. This is achieved through a variety of algorithms and statistical models. Machine learning has wide-ranging applications, from recommendation systems that suggest products to you, to medical diagnoses based on patient data, and even self-driving cars that adapt to changing road conditions. The ability to learn from data and make predictions has transformed industries and continues to shape the future of technology.\n",
    "\n",
    "Deep learning, which this guide will focus on, is a subset of machine learning that focuses on neural networks with multiple layers, known as deep neural networks. These networks are inspired by the structure and function of the human brain. Deep learning algorithms have demonstrated remarkable capabilities in recognizing complex patterns in data. They have revolutionized fields such as computer vision, natural language processing, and speech recognition. Deep learning has enabled breakthroughs in areas like image and speech recognition, language translation, and even playing complex games like Go at a superhuman level. Its ability to automatically learn hierarchical representations from data makes it a powerful tool for solving intricate problems.\n",
    "\n",
    "Do note that while this notebook will provide a guide on syntax and practical \n",
    "implementation for machine learning, this notebook is no substitute for machine\n",
    "learning classes and will still require some degree of domain knowledge. There are\n",
    "several multi-semester-long courses devoted to these very topics, with varying\n",
    "veins of mathematical and conceptual gates. Nevertheless, this guide seeks to \n",
    "attempt to simplify the process and provide some level of intutition behind the\n",
    "syntax. For a primer on neural networks and related technologies, it is highly recommended\n",
    "to check out additional resources to help you out, such as, but not limtied to\n",
    "this informative playlist from [3B1B](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&ab_channel=3Blue1Brown).\n",
    "\n",
    "Before you begin, make sure you have completed the previous steps in the starter kit,\n",
    "especially setting up your Conda environment. You'll need PyTorch and other dependencies\n",
    "installed to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Introduction to PyTorch\n",
    "\n",
    "PyTorch is an open-source machine learning library. It is known for its dynamic computation graph and flexibility, making it a preferred choice for researchers and developers. PyTorch's advantages, such as automatic differentiation, simplified methods, and extensive documentation, make it a powerful tool for deep learning projects. This guide will attempt to cover some of the main ideas, but more information can be found in the documentation [here](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "Note that certain machine learning models are pre-implemented with `scikit-learn` (sklearn). While this would allow simpler model implementations and potentially integrations with some projects, the goal of this starter kit is to provide you with the tools to design your own machine/deep learning models. Nevertheless, more information regarding `scikit-learn` can be found [here](https://scikit-learn.org/stable/tutorial/index.html) if that better suits your interests!\n",
    "\n",
    "We assume you've set up your Conda environment and installed the relevant PyTorch library. If you haven't, refer back to the [conda tutorial](1-conda.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n"
     ]
    }
   ],
   "source": [
    "# Environment Sanity Check! Run this to check that your kernel is running as expected.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if CUDA/GPU is available\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our First Model\n",
    "\n",
    "The tutorial will go through the steps of the actual implementation of a model in order to illustrate what you can implement and may need to change in your own implementation. The model that we will work through will be an classifier on the MNIST dataset (which consists of images depicts digits form 0-9 and their respective labels), which is to say, **a model that can take in a picture of a digit from 0-9 and returns a prediction of which digit the image represents** with a confidence percentage or probability. More technically, we will be implementing **logistic regression**.\n",
    "\n",
    "Consider the following cell to help visualize the images, courtesy of ChatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "# Remember, if you get an error saying that transforms is not found, you need to run the previous block with the imports!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Visualize all digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(mnist_dataset[i][0].squeeze().numpy(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {mnist_dataset[i][1]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequentially, the steps we will go through are:\n",
    "1) Data Preprocessing\n",
    "2) Model, Criterion/Loss, and Optimizer Initialization\n",
    "3) Training the Model\n",
    "4) Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "\n",
    "Data preprocessing is an important first step in the machine learning pipeline. Since the premise of machine learning is the ability to analyze trends based in data, it is necessary to find a good streamlined process for loading and processing said data. In this section we will go over the PyTorch Tensor data structure and load in the data.\n",
    "\n",
    "PyTorch Tensors are fundamental data structures in PyTorch, a popular deep learning framework. They are similar to multi-dimensional arrays or matrices and serve as the building blocks for neural networks and other numerical computations. We use PyTorch Tensors instead of nested lists because of a few unique qualities, with the most important one being support for the calculation of gradients necessary in the process of training the model. \n",
    "\n",
    "Consider the following example of converting to a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "a_list = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]     # a 2x3 list\n",
    "\n",
    "sample_tensor = torch.tensor(a_list)            # convert the list to a PyTorch tensor\n",
    "print(sample_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to convert lists into tensors, we can move directly on to loading in our data. There are many ways to load in data, whether be from parsing a CSV/TSV file, loading in a JSON/XML, etc., though the method that we will use in this guide will just use one of PyTorch's built-in datasets, the MNIST dataset. Note that all samples loaded via this method are PyTorch tensors. If you load data in another way, you may need to convert the data to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 60000 || Image Shape: torch.Size([1, 28, 28])\n",
      "Labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True) # tuple of (image, label)\n",
    "\n",
    "num_images = len(mnist_dataset)                                              # number of images in the dataset\n",
    "image_size   = mnist_dataset[0][0].shape                                     # size of the first image in the dataset\n",
    "unique_labels = set([mnist_dataset[i][1] for i in range(num_images)])        # label of the first image in the dataset\n",
    "\n",
    "# There are 60000 images of size 28x28 pixels in the training set\n",
    "print(f'Dataset Size: {num_images} || Image Shape: {image_size}')\n",
    "print(f'Labels: {unique_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with machine learning datasets, especially large ones, it's important to efficiently load and process the data for training and evaluation. PyTorch provides a convenient way to handle data loading using DataLoaders. DataLoaders allow you to load and process data in batches, making it easier to work with large datasets and perform tasks like data augmentation and shuffling.\n",
    "\n",
    "In the previous code, we loaded the MNIST dataset directly into a variable called `mnist_dataset`. While this works for small datasets like MNIST, it might not be the most efficient way to handle larger datasets. To work with larger datasets, we can use PyTorch's DataLoader class. This class provides a way to efficiently load and preprocess data in batches, which is particularly useful for tasks such as training neural networks.\n",
    "\n",
    "Let's see how to set up a DataLoader to load our MNIST dataset efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Dimensions: 48000 x torch.Size([1, 28, 28])\n",
      "Test Dataset Dimensions: 12000 x torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "def test_train_split(train_ratio = 0.8, data = mnist_dataset):\n",
    "    train_size = int(train_ratio * len(data))\n",
    "    test_size = len(mnist_dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(mnist_dataset, [train_size, test_size])\n",
    "    return train_dataset, test_dataset\n",
    "trainData, testData = test_train_split(0.8, mnist_dataset)\n",
    "\n",
    "# Sanity Check: Print the size of the training and testing sets\n",
    "print(f'Train Dataset Dimensions: {len(trainData)} x {trainData[0][0].shape}')\n",
    "print(f'Test Dataset Dimensions: {len(testData)} x {testData[0][0].shape}')\n",
    "\n",
    "# Create DataLoader instances for training and testing sets. These iterators-like objects\n",
    "# enable us to iterate over the dataset and fetch a batch of images and labels at each iteration.\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testData, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that our inputs are of size 1 x 28 x 28. This denotes that our input is a 28x28 pixel black and white image. The first dimension \"1\" denotes the number of color channels; with 1 channel, we only have light intensity (hence black and white), but with 3 channels, we can represent more colors (e.g. via channels for red, green, and blue (RGB!)). \n",
    "\n",
    "Inputs to your models do not have to be multi-dimensional tensors like a 1x28x28 input. Many inputs to neural networks are often 1D vectors with each value representing some feature of the input data. For instance, consider running a model on classifying foods with a 1D length 3 vector; the first value could represent the sugar level, the second value could represent the sodium level, and the third value could represent the number of eggs used. \n",
    "\n",
    "How can we pass a 1D vector in with image data? We can instead flatten inputs to represent them in lower dimensional spaces; for instance, we can represent our 1x28x28 tensor as a singular 1D length 784 vector. You will see later in the next section how to do this with a `Flatten()` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model, Criterion/Loss, and Optimizer Initialization\n",
    "\n",
    "In this section, we'll dive into creating and setting up the components necessary for training a machine learning model. A machine learning model can be divided into three essential components: a representation (model definition), loss (a metric to define how poorly the model is performing), and optimizer (an algorithm to help us tune the model to reduce the aforementioned loss value). We will go through each of these components individiually in order. \n",
    "\n",
    "We begin with the model's representation. We start by defining the sample architecture of our machine learning model. In this example, we'll create a simple neural network using PyTorch's `nn.Module` base class. Using the nn.Module allows for us to integrate the loss and optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output is 50.0!\n"
     ]
    }
   ],
   "source": [
    "# Our first network. It takes in a single number as input and multiplies it by a learnable parameter!\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self, multiplier):\n",
    "        super().__init__()\n",
    "        # Important! We need to wrap our learnable parameters in nn.Parameter because\n",
    "        # our optimizer will look for them and update them!\n",
    "        self.multiplier = nn.Parameter(torch.tensor(float(multiplier)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.multiplier * x\n",
    "    \n",
    "network = SimpleNetwork(5) # We want the network to mutiply inputs by 5\n",
    "\n",
    "input = 10\n",
    "output = network(input)\n",
    "print(f'Our output is {output}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the models that we seek to create will be dynamic and . We will explore more complex model architectures with multiple layers and activation functions. The architecture of a neural network greatly influences its ability to learn complex patterns from the data.\n",
    "\n",
    "Fully connected layers, also known as linear layers, are one of the fundamental building blocks of neural networks. Each neuron in an full-connected layer is connected to every neuron in the previous and subsequent layers, allowing the network to learn complex relationships in the data.\n",
    "\n",
    "In mathematical terms, let our input dimension be $k$ and our output dimension be $n$. This can be expressed by multiplying our 1xk input vector (denoted $x$) by a nxk matrix (denoted $A$), where our nx1 output $y = Ax^T$. With linear layers, our goal is to find the optimal weight matrix $A$ that will lead to the best output $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output is tensor([ 0.2370, -0.4568,  0.1987,  0.3810, -0.4424], grad_fn=<ViewBackward0>)!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Our second network. It takes in a vector input and outputs a vector after multiplying by a matrix.\n",
    "class LinearNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # No need to wrap in nn.Parameter because nn.Linear does it for us!\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "network = LinearNetwork(10, 20, 5) # We take in a 10-dimensional vector and output a 5-dimensional vector\n",
    "\n",
    "input = torch.randn(10) # Our input is a random 10-dimensional vector\n",
    "output = network(input) # We multiply the input by a matrix and output a 5-dimensional vector\n",
    "print(f'Our output is {output}!') # Running this multiple times will give you random outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've included two linear layers here. This is akin to multiplying the inputs with a first weight matrix $A$ and then multiplying that output with a second matrix $B$ (i.e. $BAx$). If you've taken a linear algebra course or are familiar with the idea of matrix properties, note that we can summarize this transformation with a single matrix $C$, i.e. $Cx$, where $C = BA$. There is no point in optimizing both matrices $B$ and $A$ since we might as well be optimizing matrix $C$ (or rather, why use two linear layers if you can have the same effect with one?). \n",
    "\n",
    "Machine learning involves trying to capture relationships in data; the approach with linear layers (i.e. $Cx$) is limited in only being able to capture linear relationships in data. We consequently introduce [_activation functions_](https://www.geeksforgeeks.org/activation-functions-neural-networks/#), transformations that introduce non-linearity to the data. An example of an activation function would be the rectified linear activation function (ReLU) defined as f(x) = max(0, x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output is tensor([-0.4606, -0.3769, -0.1019, -0.3619,  0.5110], grad_fn=<ViewBackward0>)!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NetworkWithReLU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()   # our activation function!\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "## Same structure! But now we have a ReLU activation function!\n",
    "network = NetworkWithReLU(10, 20, 5) \n",
    "\n",
    "input = torch.randn(10) \n",
    "output = network(input) \n",
    "print(f'Our output is {output}!') # The outputs look similar, but that's because we haven't trained the network yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a loss function, also known as a criterion. The loss function is a measurement of how well good your outputs are, where a higher loss value represents a worse output. Consider the loss metric Mean Squared Error (MSE). Loss is defined as the squared difference between the true output and the predicted output, which is then averaged across all samples. A unique trait about MSE emphasizes penalties on large differences between true and predicted outputs. \n",
    "\n",
    "The choice of loss function depends on the specific problem you are trying to solve. For example, for a regression problem, you might use Mean Squared Error (MSE), and for a classification problem, Cross-Entropy Loss is commonly used. \n",
    "\n",
    "Here's an example of how to define a Mean Squared Error loss function. This works well for tasks like linear regression, but we will instead use Cross-Entropy Loss for our MNIST task. Read more [here](https://www.geeksforgeeks.org/cross-entropy-cost-functions-used-in-classification/#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define our criterion here.\n",
    "# We will use Mean Squared Error (MSE) Loss. MSE is defined as:\n",
    "# MSE = (1/n) * sum((y - y_hat)^2), where y is the true label and y_hat is the\n",
    "# predicted label for each index in the output vector.\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define our optimizer. Optimizers are methods that improve your model's weights based on the loss function. Weight optimization occurs through a process known as [_backpropogation_](https://www.javatpoint.com/pytorch-backpropagation-process-in-deep-neural-network). In general, optimizers are functions that use the negative gradient of your weights with respect to the loss function (e.g. gradient descent), though the actual math behind it is quite complex. We also must consider a tunable parameter $\\alpha$, or the learning rate. This affects how much weight is put onto gradients, and is often kept around the 0.01 to 0.001 range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define our optimizer here.\n",
    "# We can use Stochastic Gradient Descent (SGD). SGD is defined as:\n",
    "# theta = theta - alpha * gradient, where theta is the learnable parameter,\n",
    "# alpha is the learning rate, and gradient is the gradient of the loss function\n",
    "# with respect to theta. On a high level, SGD is an iterative optimization\n",
    "# algorithm that updates the learnable parameters of the model in order to\n",
    "# minimize the loss function.\n",
    "\n",
    "some_model = NetworkWithReLU(10, 20, 5) \n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(some_model.parameters(), lr=learning_rate) ## Remember to pass in the parameters with respect to the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bring everything together. With respect to our task on the MNIST dataset, we\n",
    "can define a neural network that takes in images, flattens them, then returns an 1D length \n",
    "10 vector. We define the highest valued index as the predicted class.\n",
    "\n",
    "For instance, if [1, 2, 3, 2, 1, 2, 1, 1, 1, 1] is returned as the model's output,\n",
    "index 2 has the highest value, so we predict that the image represents the number 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) # flatten the image, i.e. from 1x28x28 to 784\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# We define our model here.\n",
    "input_size = 28 * 28\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "model = MNISTNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# We define our criterion here.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# We define our optimizer here.\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Model\n",
    "\n",
    "It's finally time to train our model. In this section, we will cover the training process of our machine learning model. Training is the phase where the model learns from the provided data and improves its performance over time. We'll go through epochs, loss calculation, optimization, and validation during this step. This step is admittedly more syntax than conceptual definitions, but the following code will be annotated for you for reference.\n",
    "\n",
    "We will start off with a few defintions:\n",
    "1) Epochs: in the context of training a machine learning model, an \"epoch\" refers to a single pass through the entire training dataset. During each epoch, the model sees and learns from all the training samples. The purpose of using epochs is to repeatedly expose the model to the data, allowing it to refine its parameters and improve its performance. Training for multiple epochs helps the model converge to a better solution.\n",
    "\n",
    "2) Batch Sizes: batch size determines how many training examples the model processes at once before updating its parameters. In deep learning, it's computationally efficient to update model weights based on a subset of the training data rather than the entire dataset in one go. A smaller batch size might lead to more frequent updates and faster convergence, while a larger batch size can speed up training but might result in less accurate updates.\n",
    "\n",
    "3) Optimizer Step: optimizers play a central role in training a machine learning model. After each batch of data, the model calculates the gradient of the loss with respect to its parameters. These gradients represent the direction in which the model should adjust its parameters to reduce the loss. The optimizer's job is to efficiently update the model's parameters using these gradients. Optimizers vary in terms of the learning rate and other hyperparameters that control the update process.\n",
    "\n",
    "4) Validation/Testing Step: the \"validation step\" is an essential part of the training process. It involves evaluating the model's performance on a separate dataset that it hasn't seen during training. This dataset, known as the validation/testing set was defined above in Step 1 where we split the dataset into the training and testing set. This serves as an independent measure of how well the model generalizes to new, unseen data. By monitoring the validation/testing performance during training, you can detect issues like overfitting (where the model learns to fit the training data too closely) and make adjustments to improve the model's generalization.\n",
    "\n",
    "Overall, this is a complicated step. Abundant comments will be provided in this step to explain the process, but for the most part, minorly refactoring this code and using it should be sufficient for most tasks. To further reduce loss, you can rerun the block and/or tune some hyperparameters such as epoch count, hidden layer size, learning rates, batch sizes, number of layers, and more. Make sure not to train too much otherwise the model will overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train Loss: 1.455045 // Val Loss: 0.749648 // Train Acc: 69.35% // Val Acc: 83.94%\n",
      "Epoch  2: Train Loss: 0.575593 // Val Loss: 0.482474 // Train Acc: 86.29% // Val Acc: 87.57%\n",
      "Epoch  3: Train Loss: 0.431850 // Val Loss: 0.407731 // Train Acc: 88.62% // Val Acc: 89.02%\n",
      "Epoch  4: Train Loss: 0.378244 // Val Loss: 0.372657 // Train Acc: 89.64% // Val Acc: 89.74%\n",
      "Epoch  5: Train Loss: 0.348485 // Val Loss: 0.349810 // Train Acc: 90.27% // Val Acc: 90.25%\n",
      "Epoch  6: Train Loss: 0.328145 // Val Loss: 0.334646 // Train Acc: 90.69% // Val Acc: 90.68%\n",
      "Epoch  7: Train Loss: 0.312278 // Val Loss: 0.321117 // Train Acc: 91.16% // Val Acc: 91.08%\n",
      "Epoch  8: Train Loss: 0.298970 // Val Loss: 0.310095 // Train Acc: 91.48% // Val Acc: 91.39%\n",
      "Epoch  9: Train Loss: 0.287827 // Val Loss: 0.300620 // Train Acc: 91.84% // Val Acc: 91.57%\n",
      "Epoch 10: Train Loss: 0.277619 // Val Loss: 0.292913 // Train Acc: 92.17% // Val Acc: 91.67%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10                 # number of times we iterate over the entire dataset\n",
    "total_val_losses = []       # list to store the validation losses at each epoch\n",
    "total_train_accuracies = []  # list to store training accuracies at each epoch\n",
    "total_val_accuracies = []    # list to store validation accuracies at each epoch\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    # total loss for each epoch         \n",
    "    total_loss = 0     \n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    # set the model to training mode. this is important because some layers \n",
    "    # (such as Dropout, BatchNorm) behave differently in training and testing mode.     \n",
    "    model.train()                           \n",
    "    \n",
    "    # iterate over the training set in batches of 64 images. each iteration\n",
    "    # returns a batch of 64 images and their labels.\n",
    "    for (batch_X, batch_y) in train_loader:\n",
    "        # zero out the gradients from the previous iteration. this is because\n",
    "        # pytorch accumulates gradients.\n",
    "        optimizer.zero_grad() \n",
    "        # forward pass. we get the outputs from the model using the inputs\n",
    "        # from the batch. these outputs are probabilities for each class.             \n",
    "        outputs = model(batch_X)\n",
    "        # compute the loss between the outputs and the labels in the batch.  \n",
    "        loss = criterion(outputs, batch_y)\n",
    "        # backward pass. we compute the gradients of the loss with respect to\n",
    "        # the learnable parameters of the model. \n",
    "        loss.backward()\n",
    "        # update the learnable parameters of the model using the gradients\n",
    "        # computed in the backward pass.\n",
    "        optimizer.step()\n",
    "        # add the loss of the batch to the total loss of the epoch.\n",
    "        total_loss += loss.item()\n",
    "        # calculate training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += batch_y.size(0)\n",
    "        correct_train += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation/Testing Step.\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    # We do not want to update the learnable parameters of the model when\n",
    "    # we are evaluating the model. therefore, we set the model to evaluation\n",
    "    # mode. \n",
    "    model.eval()\n",
    "\n",
    "    # We do not want to compute the gradients when we are evaluating the model.\n",
    "    # therefore, we use torch.no_grad() to disable gradient computation.\n",
    "    with torch.no_grad():\n",
    "        for (batch_X, batch_y) in test_loader:\n",
    "            # forward pass. we get the outputs from the model using the inputs\n",
    "            # from the batch. these outputs are probabilities for each class.\n",
    "            val_outputs = model(batch_X)  \n",
    "            # compute the loss between the outputs and the labels in the batch.\n",
    "            loss = criterion(val_outputs, batch_y)  \n",
    "            # add the loss of the batch to the total loss of the epoch.\n",
    "            val_loss += loss.item()\n",
    "            # calculate validation accuracy\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total_val += batch_y.size(0)\n",
    "            correct_val += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    \n",
    "    # compute the average metrics. we print these metrics for each epoch.\n",
    "    batch_train_loss = total_loss / len(train_loader)\n",
    "    batch_val_loss = val_loss / len(test_loader)\n",
    "    total_val_losses.append(batch_val_loss)\n",
    "    total_train_accuracies.append(train_accuracy)\n",
    "    total_val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # loss should go down!\n",
    "    print(f\"Epoch {epoch + 1:2d}: Train Loss: {batch_train_loss:.6f} // Val Loss: {batch_val_loss:.6f} // Train Acc: {train_accuracy:.2f}% // Val Acc: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation\n",
    "\n",
    "So, now what? We've finished training our machine learning model and we've reduced our loss values. \n",
    "But what does that really mean for us, when our original task was to predict class labels based \n",
    "on images? \n",
    "\n",
    "Let's define a helper method that returns our prediction. Coincidentally, index 0 corresponds to\n",
    "the digit 0 all the way up to 9. In different cases, we may need to create a dictionary that\n",
    "maps each index to a class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(logits):\n",
    "    return torch.argmax(logits) # return the index of the class with the highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the results. Run the following code block a few times\n",
    "and view the output images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Randomly select 3 samples from the test dataset\n",
    "sample_indices = random.sample(range(len(testData)), 3)\n",
    "\n",
    "# Create a subplot to display the samples\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for i, index in enumerate(sample_indices):\n",
    "    sample, label = testData[index]\n",
    "    # Make predictions for the sample\n",
    "    with torch.no_grad():\n",
    "        output = model(sample.unsqueeze(0))  # Add an extra dimension for batch\n",
    "\n",
    "    # Get the predicted class by finding the index with the highest probability\n",
    "    predicted_class = get_predictions(output)\n",
    "\n",
    "    # Reshape the sample for plotting\n",
    "    sample = sample.squeeze(0).numpy()\n",
    "\n",
    "    # Display the sample along with its predicted class\n",
    "    axs[i].imshow(sample, cmap='gray')\n",
    "    axs[i].set_title(f'Predicted: {predicted_class} \\nActual: {label}')\n",
    "    axs[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the predicted values don't always match up with the actual values. The reason why is because the model that we've created is relatively simple. More modern methods use different layer types (e.g. convolutional layers, which are better geared for image classification tasks), different layer counts, different optimizers (e.g. RMSProp, Adam, and other modern gradient-based functions), different epoch counts, and many more. While our training loop achieved an accuracy of roughly 90%, modern methods have achieved a roughly 99.5% accuracy.\n",
    "\n",
    "That being said, a 90% accuracy is nothing to scoff at. Random guessing would only achieve around an accuracy of 10%. For reference, try re-intializing your model (re-running the final block in Step 2) and running this visualization block, and see how many are classified correctly. With these steps, you've successfully trained your first model! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack@brown",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
